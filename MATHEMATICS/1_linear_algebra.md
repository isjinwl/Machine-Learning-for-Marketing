>**Neural Network and Deep Learning(邱锡鹏，2020)**[[Book]](https://nndl.github.io/)<br/>
> #### Appendix.A 线性代数(Linear Algebra)
> + N维向量
>    + 标量(Scalar)：是一个实数，只有大小，没有方向。e.g.,*a,b,c,...*
>    + 向量(Vector)：是一组实数构成的有序数组，有大小，有方向.e.g.,N维向量<!-- $a=[a_1, a_2, ..., a_N].$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a%3D%5Ba_1%2C%20a_2%2C%20...%2C%20a_N%5D"> 
>    + 向量空间(Vector Space or Linear Space)：向量组成的集合，且满足*向量加法*和*标量乘法*（两者可视为向量空间的条件）.
>       + 欧氏空间(Euclidean Space)：一个欧氏空间通常表示为<!-- $\mathbb{R}^N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathbb%7BR%7D%5EN">，N为空间维度(Dimension).
>          + 欧氏空间中的*向量加法*: <br/><!-- $a+b=[a_1, a_2, ..., a_N]+[b_1, b_2, ..., b_N]=[a_1+b_1,a_2+b_2,...,a_N+b_N].$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a%2Bb%3D%5Ba_1%2C%20a_2%2C%20...%2C%20a_N%5D%2B%5Bb_1%2C%20b_2%2C%20...%2C%20b_N%5D%3D%5Ba_1%2Bb_1%2Ca_2%2Bb_2%2C...%2Ca_N%2Bb_N%5D"> 
>          + 欧氏空间中的*标量乘法*：<br/><!-- $c \cdot a = c \cdot [a_1, a_2, ..., a_N]=[ca_1, ca_2, ..., ca_N].$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=c%20%5Ccdot%20a%20%3D%20c%20%5Ccdot%20%5Ba_1%2C%20a_2%2C%20...%2C%20a_N%5D%3D%5Bca_1%2C%20ca_2%2C%20...%2C%20ca_N%5D."> 其中<!-- $a,b,c \in \mathbb{R}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a%2Cb%2Cc%20%5Cin%20%5Cmathbb%7BR%7D"> 属于标量.
>       + 线性子空间：向量空间的一个子集，也满足*向量加法*和*标量乘法*.
>       + 线性相关(linearly dependent)：如果向量空间中的一个向量可以用有限个其他向量的线性组合表示，则称线性相关.<br/>或线性空间<!-- $\mathbb{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathbb%7Bv%7D">中的**M**个向量<!-- $\lbrace v_1, v_2,...,v_M \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%7Bv_1%2C%20v_2%2C...%2Cv_M%7D">，如果<!-- $\lambda_1 v_1+\lambda_2 v_2+...+\lambda_M v_M=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clambda_1%20v_1%2B%5Clambda_2%20v_2%2B...%2B%5Clambda_M%20v_M%3D0">存在非零解<!-- $\lambda_1,\lambda_2,...,\lambda_M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clambda_1%2C%5Clambda_2%2C...%2C%5Clambda_M">（标量），则向量组<!-- $\lbrace v_1, v_2,...,v_M \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clbrace%20v_1%2C%20v_2%2C...%2Cv_M%20%5Crbrace">是线性相关的；否则向量组<!-- $\lbrace v_1, v_2,...,v_M \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clbrace%20v_1%2C%20v_2%2C...%2Cv_M%20%5Crbrace">是线性无关的(linearly independent).
>        + 基向量：N维向量空间<!-- $\mathcal{V}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BV%7D">的基(Base)<!-- $\mathcal{B}=\lbrace e_1,e_2,...,e_N \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BB%7D%3D%5Clbrace%20e_1%2Ce_2%2C...%2Ce_N%20%5Crbrace">是<!-- $\mathbb{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathbb%7Bv%7D">的有限子集，其元素之间线性无关。向量空间<!-- $\mathbb{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathbb%7Bv%7D">的所有向量都可以按唯一的方式表达为<!-- $\mathcal{B}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BB%7D">中向量的线性组合，即对于任意<!-- $v \in \mathcal{V}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v%20%5Cin%20%5Cmathcal%7BV%7D">存在一组标量<!-- $\lbrace \lambda_1,\lambda_2,...,\lambda_N \rbrace$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clbrace%20%5Clambda_1%2C%5Clambda_2%2C...%2C%5Clambda_N%20%5Crbrace">，使得<!-- $v=\lambda_1e_1+\lambda_2e_2+...+\lambda_Ne_N.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v%3D%5Clambda_1e_1%2B%5Clambda_2e_2%2B...%2B%5Clambda_Ne_N"> 且如果基向量是有序的，则<!-- $(\lambda_1,\lambda_2,...,\lambda_N)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(%5Clambda_1%2C%5Clambda_2%2C...%2C%5Clambda_N)">称为向量<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">关于基<!-- $\mathcal{B}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BB%7D">的坐标.如果基向量是Standard Basis,<!-- $(\lambda_1,\lambda_2,...,\lambda_N)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(%5Clambda_1%2C%5Clambda_2%2C...%2C%5Clambda_N)">则可称为向量<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">的笛卡尔坐标(Cartesian Coordinate).
>        + 内积(Inner Product|Dot Product|Scalar Product)：一个N维向量空间中的2个向量***a***和***b***的内积为<!-- $<a,b>=\sum_{n-1}^Na_nb_n$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%3Ca%2Cb%3E%3D%5Csum_%7Bn-1%7D%5ENa_nb_n">(基向量相同,<!-- $cos\theta=1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=cos%5Ctheta%3D1">).
>        + 正交(Orthogonal)：如果同一个向量空间中2个向量的内积为0，则这两个向量正交；如果向量空间A中的一个向量<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">与子空间<!-- $\mathcal{U}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BU%7D">的每个向量正交，那么向量<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">与子空间<!-- $\mathcal{U}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BU%7D">正交.
>    + 范数(Norm)：一个表示向量“长度”的函数，为向量空间内的所有向量赋予*非零*的正长度或大小.对于一个N维向量<!-- $\mathcal{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bv%7D">, 常见的一个范数函数<!-- $\mathcal{l}_p$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_p">范数，<!-- $\mathcal{l}_p(\mathcal{v}) = \begin{Vmatrix} \mathcal{v} \end{Vmatrix}_p=(\sum_{n=1}^N\begin{vmatrix}v_n\end{vmatrix}^p)^{\frac 1p}$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\icrSLi5ixn.svg">. 其中<!-- $p\ge 0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=p%5Cge%200">为这个标量的参数，常见取值有<!-- $1,2,\infty$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=1%2C2%2C%5Cinfty">.
>        + <!-- $\mathcal{l}_1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_1">范数：<!-- $\begin{Vmatrix} \mathcal{v} \end{Vmatrix}_1=\sum_{n=1}^N \begin{vmatrix} \mathcal{v}_n \end{vmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7BVmatrix%7D%20%5Cmathcal%7Bv%7D%20%5Cend%7BVmatrix%7D_1%3D%5Csum_%7Bn%3D1%7D%5EN%20%5Cbegin%7Bvmatrix%7D%20%5Cmathcal%7Bv%7D_n%20%5Cend%7Bvmatrix%7D."> 向量的各个元素的绝对值之和.
>        + <!-- $\mathcal{l}_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_2">范数：<br/>
>           <!-- $\begin{Vmatrix} \mathcal{v} \end{Vmatrix}_2=\sqrt{\sum_{n=1}^N \mathcal{v}_n^2}=\sqrt{\mathcal{v}^T\mathcal{v}}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7BVmatrix%7D%20%5Cmathcal%7Bv%7D%20%5Cend%7BVmatrix%7D_2%3D%5Csqrt%7B%5Csum_%7Bn%3D1%7D%5EN%20%5Cmathcal%7Bv%7D_n%5E2%7D%3D%5Csqrt%7B%5Cmathcal%7Bv%7D%5ET%5Cmathcal%7Bv%7D%7D."> 向量的各个元素的平方和再开平方. <!-- $\mathcal{l}_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_2">范数又称Euclidean范数或者Frobenius范数或者向量的*模*.从几何角度，向量是从原点出发的一个带箭头的有向线段，<!-- $\mathcal{l}_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_2">范数是线段的长度。
>        + <!-- $\mathcal{l}_\infty$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_%5Cinfty">范数：
>          <!-- $\begin{Vmatrix}\mathcal{v}\end{Vmatrix}_\infty=max \lbrace v_1,v_2,...,v_N \rbrace.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7BVmatrix%7D%5Cmathcal%7Bv%7D%5Cend%7BVmatrix%7D_%5Cinfty%3Dmax%20%5Clbrace%20v_1%2Cv_2%2C...%2Cv_N%20%5Crbrace."> 向量的各个元素的*最大绝对值*.
>    + 常见向量：
>       + 全0向量：所有元素都为0的向量，表示为0，是笛卡尔坐标系中的原点.
>       + 全1向量：所有元素都为1的向量，表示为1.
>       + one-hot向量：有且只有一个元素为1，其他元素都为0的向量.
> + 矩阵
>    + 线性映射(Linear Mapping)：即线性变换，可理解为一个映射函数，把*向量*从线性空间<!-- $\mathcal{X}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BX%7D">映射到线性空间<!-- $\mathcal{Y}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BY%7D">，表示为<!-- $f:\mathcal{X} \to \mathcal{Y}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f%3A%5Cmathcal%7BX%7D%20%5Cto%20%5Cmathcal%7BY%7D">，并满足：<br/>
> 对于<!-- $\mathcal{X}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BX%7D">中任意两个向量<!-- $u$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=u">和<!-- $v$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=v">，以及任意标量<!-- $c$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=c">，有<!-- $f(u+v)=f(u)+f(v), f(c \cdot v)=cf(v)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f(u%2Bv)%3Df(u)%2Bf(v)%2C%20f(c%20%5Ccdot%20v)%3Dcf(v)">.
>       + 例：函数<!-- $f:\mathbb{R}^N \to \mathbb{R}^M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f%3A%5Cmathbb%7BR%7D%5EN%20%5Cto%20%5Cmathbb%7BR%7D%5EM"><br/><!-- $y=Ax={\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1N}\\a_{21} & a_{22} & \cdots & a_{2N}\\\vdots & \vdots & \ddots & \vdots\\a_{M1} & a_{M2} &  \cdots & a_{MN}\end{bmatrix}}\times{\begin{bmatrix}x_{1}\\x_{2}\\\vdots\\x_N\end{bmatrix}}={\begin{bmatrix}a_{11}x_1+a_{12}x_2+\cdots+a_{1N}x_N\\a_{21}x_1+a_{22}x_2+\cdots+a_{2N}x_N\\\vdots\\a_{M1}x_1+a_{M2}x_2+\cdots+a_{MN}x_N\end{bmatrix}}={\begin{bmatrix}y_1\\y_2\\\vdots\\y_M\end{bmatrix}}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=y%3DAx%3D%7B%5Cbegin%7Bbmatrix%7Da_%7B11%7D%20%26%20a_%7B12%7D%20%26%20%5Ccdots%20%26%20a_%7B1N%7D%5C%5Ca_%7B21%7D%20%26%20a_%7B22%7D%20%26%20%5Ccdots%20%26%20a_%7B2N%7D%5C%5C%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%5C%5Ca_%7BM1%7D%20%26%20a_%7BM2%7D%20%26%20%20%5Ccdots%20%26%20a_%7BMN%7D%5Cend%7Bbmatrix%7D%7D%5Ctimes%7B%5Cbegin%7Bbmatrix%7Dx_%7B1%7D%5C%5Cx_%7B2%7D%5C%5C%5Cvdots%5C%5Cx_N%5Cend%7Bbmatrix%7D%7D%3D%7B%5Cbegin%7Bbmatrix%7Da_%7B11%7Dx_1%2Ba_%7B12%7Dx_2%2B%5Ccdots%2Ba_%7B1N%7Dx_N%5C%5Ca_%7B21%7Dx_1%2Ba_%7B22%7Dx_2%2B%5Ccdots%2Ba_%7B2N%7Dx_N%5C%5C%5Cvdots%5C%5Ca_%7BM1%7Dx_1%2Ba_%7BM2%7Dx_2%2B%5Ccdots%2Ba_%7BMN%7Dx_N%5Cend%7Bbmatrix%7D%7D%3D%7B%5Cbegin%7Bbmatrix%7Dy_1%5C%5Cy_2%5C%5C%5Cvdots%5C%5Cy_M%5Cend%7Bbmatrix%7D%7D."><br/><!-- $(A \in \mathbb{R}^{M \times N}, x \in \mathbb{R}^N, y \in \mathbb{R}^M).$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%20x%20%5Cin%20%5Cmathbb%7BR%7D%5EN%2C%20y%20%5Cin%20%5Cmathbb%7BR%7D%5EM).">  
>    + 仿射变换(Affine Transformation)：通过一个线性变换和一个平移，实现一个线性空间的旋转、平移、缩放变换. 表示为：<br/><!-- $y=Ax+b, A \in \mathbb{R}^{N \times N},x \in \mathbb{R}^N,b \in \mathbb{R}^N.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=y%3DAx%2Bb%2C%20A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20N%7D%2Cx%20%5Cin%20%5Cmathbb%7BR%7D%5EN%2Cb%20%5Cin%20%5Cmathbb%7BR%7D%5EN.">
>       + 当b=0时，*仿射变换*就退化为*线性变换*.
>       + 仿射变换不会改变原始空间中的相对位置关系，具有4个性质：<br/>1. 共线性：同一条直线上的三个及以上的点变换后依然在一条直线上.<br/>2. 比例不变：不同点之间的距离比例不变.<br/>3. 平行性不变：两条平行线在转化后依然会平行.<br/>4. 凸性不变：凸集(Convex set)转换后依然是凸的.
>          + 凸性: <!-- $\forall {x_1,x_2} \in C,\forall \theta \in [0,1],\theta x_1+(1-\theta)x_2 \in C.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cforall%20%7Bx_1%2Cx_2%7D%20%5Cin%20C%2C%5Cforall%20%5Ctheta%20%5Cin%20%5B0%2C1%5D%2C%5Ctheta%20x_1%2B(1-%5Ctheta)x_2%20%5Cin%20C.">如果集合C中任意两点之间的线段都在C中，则该集合是凸的.<br/>
\* 凸优化的延伸介绍(今天数学学点啥，2020)[[知乎]](https://zhuanlan.zhihu.com/p/94879910?from_voters_page=true)
>    + 矩阵操作：
>       + 加：<!-- $A,B \in \mathbb{R}^{M \times N},[A+B]_{mn}=a_{mn}+b_{mn}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%2CB%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%5BA%2BB%5D_%7Bmn%7D%3Da_%7Bmn%7D%2Bb_%7Bmn%7D.">
>       + 乘积：两个矩阵的乘积仅当第一个矩阵的*列数*和第二个矩阵的*行数*相等时才能定义，如：<!-- $A \in \mathbb{R}^{M \times K},B \in \mathbb{R}^{K \times N}, then AB \in \mathbb{R}^{M \times N},[AB]_{mn}=\sum_{k=1}^K a_{mk}b_{kn}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20K%7D%2CB%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BK%20%5Ctimes%20N%7D%2C%20then%20AB%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%5BAB%5D_%7Bmn%7D%3D%5Csum_%7Bk%3D1%7D%5EK%20a_%7Bmk%7Db_%7Bkn%7D.">
>          + 矩阵乘法满足*结合律*和*分配律*. 如：<br/>用矩阵A和B分别表示线性映射<!-- $f:\mathbb{R}^N \to \mathbb{R}^K, g:\mathbb{R}^K \to \mathbb{R}^M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=f%3A%5Cmathbb%7BR%7D%5EN%20%5Cto%20%5Cmathbb%7BR%7D%5EK%2C%20g%3A%5Cmathbb%7BR%7D%5EK%20%5Cto%20%5Cmathbb%7BR%7D%5EM">,则<!-- $(g \circ f)(x)=g(f(x))=g(Bx)=A(Bx)=(AB)x.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(g%20%5Ccirc%20f)(x)%3Dg(f(x))%3Dg(Bx)%3DA(Bx)%3D(AB)x.">     
>       + 转置(Transposition)：<!-- $A \in \mathbb{R}^{M \times N} \to A^T \in \mathbb{R}^{N \times M}, [A]_{mn}=[A^T]_{nm}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%20%5Cto%20A%5ET%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20M%7D%2C%20%5BA%5D_%7Bmn%7D%3D%5BA%5ET%5D_%7Bnm%7D.">
>       + 逐点乘积(Hadamard Product)：<!-- $A \in \mathbb{R}^{M \times N},B \in \mathbb{R}^{M \times N},A \bigodot B \in \mathbb{R}^{M \times N},[A \bigodot B]_{mn}=a_{mn}b_{mn},[cA]_{mn}=ca_{mn}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2CB%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2CA%20%5Cbigodot%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%5BA%20%5Cbigodot%20B%5D_%7Bmn%7D%3Da_%7Bmn%7Db_%7Bmn%7D%2C%5BcA%5D_%7Bmn%7D%3Dca_%7Bmn%7D."> 若矩阵A和B的维度不相等，则Hadamard积没有定义.
>      + Kronecker积(Kronecker Product)：<br/><!-- $A \in \mathbb{R}^{M \times N}, B \in \mathbb{R}^{S \times T},[A \bigotimes B]=\begin{bmatrix}a_{11}B & a_{12}B &\cdots & a_{1N}B\\a_{21}B & a_{22}B & \cdots & a_{2N}B\\ \vdots & \vdots & \ddots & \vdots\\a_{M1}B & a_{M2}B & \cdots & a_{MN}B\end{bmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2C%20B%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BS%20%5Ctimes%20T%7D%2C%5BA%20%5Cbigotimes%20B%5D%3D%5Cbegin%7Bbmatrix%7Da_%7B11%7DB%20%26%20a_%7B12%7DB%20%26%5Ccdots%20%26%20a_%7B1N%7DB%5C%5Ca_%7B21%7DB%20%26%20a_%7B22%7DB%20%26%20%5Ccdots%20%26%20a_%7B2N%7DB%5C%5C%20%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%5C%5Ca_%7BM1%7DB%20%26%20a_%7BM2%7DB%20%26%20%5Ccdots%20%26%20a_%7BMN%7DB%5Cend%7Bbmatrix%7D.">适用于两个任意大小的矩阵相乘，且通常不符合交换律：<!-- $A \bigotimes B$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cbigotimes%20B">不同于<!-- $B \bigotimes A$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=B%20%5Cbigotimes%20A">.
>      + 外积(Outer Product)：<!-- $a \in \mathbb{R}^M,b \in \mathbb{R}^N,a \bigotimes b=a \times b^T=\begin{bmatrix}a_1 \\ a_2 \\ \cdots \\ a_M\end{bmatrix}\begin{bmatrix}b_1 & b_2 & \cdots & b_N\end{bmatrix}=\begin{bmatrix}a_1b_1 & a_1b_2 & \cdots & a_1b_N\\a_2b_1 & a_2b_2 & \cdots & a_2b_N\\\vdots & \vdots & \ddots & \vdots\\a_Mb_1 & a_Mb_2 & \cdots & a_Mb_N\end{bmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=a%20%5Cin%20%5Cmathbb%7BR%7D%5EM%2Cb%20%5Cin%20%5Cmathbb%7BR%7D%5EN%2Ca%20%5Cbigotimes%20b%3Da%20%5Ctimes%20b%5ET%3D%5Cbegin%7Bbmatrix%7Da_1%20%5C%5C%20a_2%20%5C%5C%20%5Ccdots%20%5C%5C%20a_M%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Db_1%20%26%20b_2%20%26%20%5Ccdots%20%26%20b_N%5Cend%7Bbmatrix%7D%3D%5Cbegin%7Bbmatrix%7Da_1b_1%20%26%20a_1b_2%20%26%20%5Ccdots%20%26%20a_1b_N%5C%5Ca_2b_1%20%26%20a_2b_2%20%26%20%5Ccdots%20%26%20a_2b_N%5C%5C%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%5C%5Ca_Mb_1%20%26%20a_Mb_2%20%26%20%5Ccdots%20%26%20a_Mb_N%5Cend%7Bbmatrix%7D"> <br/><!-- $[a \bigotimes b]_{mn}=a_mb_n.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Ba%20%5Cbigotimes%20b%5D_%7Bmn%7D%3Da_mb_n."><br/>Kronecker积的特例(因此$\bigotimes$既可以表示Kronecker积也可以表示外积)，在线性代数中一般指*两个向量*的张量积，其结果为*一个矩阵*;与内积的区别在于内积结果为*标量*.
>      + 向量化(Vectorization)：<!-- $A=[a_{ij}]_{MN},vec(A)=[a_{11},a_{21},\cdots,a_{M1},a_{12},a_{22},\cdots,a_{M2},\cdots,a_{1N},a_{2N},\cdots,a_{MN}]^T.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3D%5Ba_%7Bij%7D%5D_%7BMN%7D%2Cvec(A)%3D%5Ba_%7B11%7D%2Ca_%7B21%7D%2C%5Ccdots%2Ca_%7BM1%7D%2Ca_%7B12%7D%2Ca_%7B22%7D%2C%5Ccdots%2Ca_%7BM2%7D%2C%5Ccdots%2Ca_%7B1N%7D%2Ca_%7B2N%7D%2C%5Ccdots%2Ca_%7BMN%7D%5D%5ET.">矩阵的向量化是将一个矩阵表示为一个*列向量*.
>      + 迹：方块矩阵(<!-- $N \times N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=N%20%5Ctimes%20N">)A的对角线元素之和,<!-- $tr(A)=a_{11}+a_{22}+\cdots+a_{NN}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=tr(A)%3Da_%7B11%7D%2Ba_%7B22%7D%2B%5Ccdots%2Ba_%7BNN%7D.">矩阵乘法不满足交换律，但迹相同，<!-- $tr(AB)=tr(BA)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=tr(AB)%3Dtr(BA)">.
>      + 行列式(Determinant)：(几何意义)<br/>1. 从静态的体积概念来说是行列式中的行或列向量所构成的超平行多面体的有向面积或有向体积；<br/>2. 从动态的变换比例概念来说矩阵A的行列式det(A)就是线性变换A下的图形面积或体积(相对于单位面积或体积:正方形/正方体/超立方体,容积等于1）的**伸缩因子**(像域容积/原域容积)。
>        + 二阶行列式的几何性质(其行或列向量所张成的**有向**平行四边形面积)：
>           + <!-- $k\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}=\begin{vmatrix}ka_1&ka_2\\b_1&b_2\end{vmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=k%5Cbegin%7Bvmatrix%7Da_1%26a_2%5C%5Cb_1%26b_2%5Cend%7Bvmatrix%7D%3D%5Cbegin%7Bvmatrix%7Dka_1%26ka_2%5C%5Cb_1%26b_2%5Cend%7Bvmatrix%7D.">
>           + <!-- $\begin{vmatrix}a_1&a_2\\b_1+c_1&b_2+c_2\end{vmatrix}=\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}+\begin{vmatrix}a_1&a_2\\c_1&c_2\end{vmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7Bvmatrix%7Da_1%26a_2%5C%5Cb_1%2Bc_1%26b_2%2Bc_2%5Cend%7Bvmatrix%7D%3D%5Cbegin%7Bvmatrix%7Da_1%26a_2%5C%5Cb_1%26b_2%5Cend%7Bvmatrix%7D%2B%5Cbegin%7Bvmatrix%7Da_1%26a_2%5C%5Cc_1%26c_2%5Cend%7Bvmatrix%7D.">
>           + <!-- $\begin{vmatrix}a_1&a_2\\ka_1&ka_2\end{vmatrix}=0.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7Bvmatrix%7Da_1%26a_2%5C%5Cka_1%26ka_2%5Cend%7Bvmatrix%7D%3D0.">
>           + <!-- $\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}=-\begin{vmatrix}b_1&b_2\\a_1&a_2\end{vmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7Bvmatrix%7Da_1%26a_2%5C%5Cb_1%26b_2%5Cend%7Bvmatrix%7D%3D-%5Cbegin%7Bvmatrix%7Db_1%26b_2%5C%5Ca_1%26a_2%5Cend%7Bvmatrix%7D.">
>           + <!-- $\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}=\begin{vmatrix}a_1&b_1\\a_2&b_2\end{vmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7Bvmatrix%7Da_1%26a_2%5C%5Cb_1%26b_2%5Cend%7Bvmatrix%7D%3D%5Cbegin%7Bvmatrix%7Da_1%26b_1%5C%5Ca_2%26b_2%5Cend%7Bvmatrix%7D.">
>        + 三阶行列式的几何性质(其行或列向量所张成的平行六面体的**有向**体积)：
>           + <!-- $det(a,b,c+d)=det(a,b,c)+det(a,b,d)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(a%2Cb%2Cc%2Bd)%3Ddet(a%2Cb%2Cc)%2Bdet(a%2Cb%2Cd)">
>           + <!-- $det(a,a,c)=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(a%2Ca%2Cc)%3D0"> <br/>(相当于三维空间中六面体被压成了高度为零的二维平面,这个平面的三维体积为零.)
>           + <!-- $det(a,b,c)=-det(b,a,c)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(a%2Cb%2Cc)%3D-det(b%2Ca%2Cc)">(改变方向.)
>           + <!-- $kdet(a,b,c)=det(ka,b,c)=det(a,kb,c)=det(a,b,kc)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=kdet(a%2Cb%2Cc)%3Ddet(ka%2Cb%2Cc)%3Ddet(a%2Ckb%2Cc)%3Ddet(a%2Cb%2Ckc)">
>           + <!-- $det(a,b,c+ka)=det(a,b,c)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(a%2Cb%2Cc%2Bka)%3Ddet(a%2Cb%2Cc)">
>           + <!-- $det A = det A'$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det%20A%20%3D%20det%20A'">
>        + 行列式的乘积项:由各个坐标轴上的有向线段所围起来的所有有向面积或有向体积的累加和。这个累加要注意每个面积或体积的方向或符号，方向相同的要加，方向相反的要减(叉积右手定则)，因而，这个累加的和是代数和.
>          + 二阶行列式的乘积项：(可以拆解成2个2维有向图形)<br/><!-- $\begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}=\begin{vmatrix}a_1&0\\0&b_2\end{vmatrix}+\begin{vmatrix}0&a_2\\b_1&0\end{vmatrix}=a_1b_2-a_2b_1.$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\MPJl2WqQGF.svg">
>          + 三阶行列式的乘积项：(可以拆解成6个3维有向图形)<br/><!-- $\begin{vmatrix}a_1&a_2&a_3\\b_1&b_2&b_3\\c_1&c_2&c_3\end{vmatrix}=a_1\begin{vmatrix}b_2&b_3\\c_2&c_3\end{vmatrix}-a_2\begin{vmatrix}b_1&b_3\\c_1&c_3\end{vmatrix}+a_3\begin{vmatrix}b_1&b_2\\c_1&c_2\end{vmatrix}=a_1b_2c_3+a_2b_3c_1+a_3b_1c_2-a_1b_3c_2-a_2b_1c_3-a_3b_2c_1.$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\Il8r4B8JSN.svg">
>          + n阶行列式的乘积项：(可以拆解成n!个n维有向图形)<br/><!--$det(A)=\begin{vmatrix}a_{11}&a_{12}&\cdots&a_{1n}\\a_{21}&a_{22}&\cdots&a_{2n}\\\vdots&\vdots&\ddots&\vdots\\a_{n1}&a_{n2}&\cdots&a_{nn}\end{vmatrix}=\sum_{j=1}^n(-1)^{i+j}A_{i,j}M_{i,j}(Laplace Expansion)=\sum_{(j_1,j_2,\cdots,j_n)}(-1)^{t}a_{1j_1}a_{2j_2}\cdots a_{nj_n}=\sum_{\sigma \in S_n}sgn(\sigma)\Pi_{i=1}^na_{i,\sigma(i)}.$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\LMKNBgEmUr.svg"><br/>ps1: where <!-- $B_{i,j}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=B_%7Bi%2Cj%7D"> is the entry of the ith row and jth column of A, and <!-- $M_{i,j}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=M_%7Bi%2Cj%7D"> is the determinant of the submatrix obtained by removing the ith row and the jth column of A.The term <!-- $(-1)^{i+j}M_{i,j}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=(-1)%5E%7Bi%2Bj%7DM_%7Bi%2Cj%7D"> is called the cofactor of <!-- $A_{i,j}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A_%7Bi%2Cj%7D"> in A.<br/>ps2: <!-- $sgn(\sigma)=(-1)^{N(\sigma)} \in \{-1,1\},N(\sigma)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=sgn(%5Csigma)%3D(-1)%5E%7BN(%5Csigma)%7D%20%5Cin%20%5C%7B-1%2C1%5C%7D%2CN(%5Csigma)">表示σ中反向对的个数，N(σ)=偶数则sgn(σ)=1,N(σ)=奇数则sgn(σ)=-1.<!-- $\sigma \in S_n, S_n=\{1,2,\cdots,n\}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Csigma%20%5Cin%20S_n%2C%20S_n%3D%5C%7B1%2C2%2C%5Ccdots%2Cn%5C%7D."><br/>\*行列式的几何意义(AndyJee, 2013)[[Blog]](https://www.cnblogs.com/AndyJee/p/3491487.html)<br/>\*线性代数的本质(3Blue1Brown,2017)[[Video]](https://www.bilibili.com/video/BV1ys411472E?p=8)<br/>\*Laplace Expansion[[Wiki]](https://en.wikipedia.org/wiki/Laplace_expansion)
>      + 秩(Rank)：means the number of dimensions in the output of a transformation, or the number of dimensions in the column space.如,2×2矩阵的秩最大为2，基向量依旧能张成整个二维空间且矩阵的行列式不为0；但对于3×3矩阵来说，秩=2意味着空间被压缩了,其行列式=0；当秩达到最大值时(<!-- $A \in \mathbb{R}^{M \times N},rank(A)=min(M,N)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D%2Crank(A)%3Dmin(M%2CN)">)，秩=列数(满秩)，<br/>\*线性代数的本质(3Blue1Brown,2017)[[Video]](https://www.bilibili.com/video/BV1ys411472E?p=8)
>      + (矩阵)范数：单个N维向量的<!-- $\mathcal{l}_p$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_p">范数<!-- $\to \mathcal{l}_p(\mathcal{v}) = \begin{Vmatrix} \mathcal{v} \end{Vmatrix}_p=(\sum_{n=1}^N\begin{vmatrix}v_n\end{vmatrix}^p)^{\frac 1p}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cto%20%5Cmathcal%7Bl%7D_p(%5Cmathcal%7Bv%7D)%20%3D%20%5Cbegin%7BVmatrix%7D%20%5Cmathcal%7Bv%7D%20%5Cend%7BVmatrix%7D_p%3D(%5Csum_%7Bn%3D1%7D%5EN%5Cbegin%7Bvmatrix%7Dv_n%5Cend%7Bvmatrix%7D%5Ep)%5E%7B%5Cfrac%201p%7D"><br/>矩阵<!-- $A \in \mathbb{R}^{M \times N}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20N%7D">的<!-- $\mathcal{l}_p$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cmathcal%7Bl%7D_p">范数<!-- $\to \begin{Vmatrix}A\end{Vmatrix}_p=(\sum_{m=1}^M \sum_{n=1}^N \begin{vmatrix}a_{mn}\end{vmatrix}^p)^{\frac 1p}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cto%20%5Cbegin%7BVmatrix%7DA%5Cend%7BVmatrix%7D_p%3D(%5Csum_%7Bm%3D1%7D%5EM%20%5Csum_%7Bn%3D1%7D%5EN%20%5Cbegin%7Bvmatrix%7Da_%7Bmn%7D%5Cend%7Bvmatrix%7D%5Ep)%5E%7B%5Cfrac%201p%7D.">
>   + 矩阵类型：
>      + 对称矩阵(Symmetric Matrix)：$A=A^T$,其转置等于自己的矩阵.实系数矩阵与自己的转置相乘后是实对称矩阵，而且实对称矩阵必定可以分解出特征值与特征向量。
>      + 对角矩阵(Diagonal Matrix)：<!-- $[A]_{mn}=0,\forall m,n \in [1,\cdots,N],and m \neq n.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5BA%5D_%7Bmn%7D%3D0%2C%5Cforall%20m%2Cn%20%5Cin%20%5B1%2C%5Ccdots%2CN%5D%2Cand%20m%20%5Cneq%20n.">除主对角线之外的元素都为0的矩阵，通常指方块矩阵.
>      + 单位矩阵(Identity Matrix)：<br/><!-- $I_N=diag(1,1,\cdots,1)=\begin{bmatrix}1&0&\cdots&0\\0&1&\cdots&0\\\vdots&\vdots&\ddots&0\\0&0&\cdots&1\end{bmatrix},AI_N=I_MA=A(A \in \mathbb{R}^{M \times M}).$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=I_N%3Ddiag(1%2C1%2C%5Ccdots%2C1)%3D%5Cbegin%7Bbmatrix%7D1%260%26%5Ccdots%260%5C%5C0%261%26%5Ccdots%260%5C%5C%5Cvdots%26%5Cvdots%26%5Cddots%260%5C%5C0%260%26%5Ccdots%261%5Cend%7Bbmatrix%7D%2CAI_N%3DI_MA%3DA(A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BM%20%5Ctimes%20M%7D)."><br/>特殊的对角矩阵，其主对角线元素为1，其余为0.
>      + 逆矩阵(Inverse Matrix)：<br/><!-- $det(A) \neq 0, then A^{-1} exist. A \in \mathbb{R}^{N \times N},AA^{-1}=I_N.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=det(A)%20%5Cneq%200%2C%20then%20A%5E%7B-1%7D%20exist.%20A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20N%7D%2CAA%5E%7B-1%7D%3DI_N.">如果存在另一个方块矩阵B使<!-- $AB=BA=I_N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=AB%3DBA%3DI_N">,则称A是*可逆的*，矩阵B称为A的逆矩阵，记作<!-- $A^{-1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%5E%7B-1%7D">.
>      + 正定矩阵(Positive-Definite Matrix)：
>      + 正交矩阵(Orthogonal Matrix)：<!-- $A \in \mathbb{R}^{N \times N},A^T=A^{-1}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20N%7D%2CA%5ET%3DA%5E%7B-1%7D.">正交矩阵A的转置矩阵等于其逆矩阵.如果矩阵是单位正交矩阵，<!-- $A \times A^T=A \times A^{-1}=I$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%20%5Ctimes%20A%5ET%3DA%20%5Ctimes%20A%5E%7B-1%7D%3DI">.
>      + Gram矩阵(Gram Matrix)：
>   + 特征值与特征向量(Eigenvalue & Eigenvector)：<!-- $A\ \in \mathbb{R}^{N \times N},A \vec{v}=\lambda \vec{v}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%5C%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20N%7D%2CA%20%5Cvec%7Bv%7D%3D%5Clambda%20%5Cvec%7Bv%7D."><br/>特征向量<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">在矩阵A的作用下，保持方向不变进行比例为λ的伸缩.从$A \vec{v}$相对于<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">伸缩可以看出，特征向量分别对应特征值λ＞1或λ＜1.<br/>特征值λ和特征向量<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">求解示例：<br/><!-- $Ax={\lambda x \Rightarrow Ax=\lambda Ex \Rightarrow (\lambda E-A)x}=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=Ax%3D%7B%5Clambda%20x%20%5CRightarrow%20Ax%3D%5Clambda%20Ex%20%5CRightarrow%20(%5Clambda%20E-A)x%7D%3D0"><br/><!-- $A=\begin{bmatrix}e&2&-5\\6&4&-9\\5&3&-7\end{bmatrix},|\lambda E-A|=\begin{vmatrix}\lambda -4&-2&5\\-6&\lambda-4&9\\-5&-3&\lambda+7\end{vmatrix}=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3D%5Cbegin%7Bbmatrix%7De%262%26-5%5C%5C6%264%26-9%5C%5C5%263%26-7%5Cend%7Bbmatrix%7D%2C%7C%5Clambda%20E-A%7C%3D%5Cbegin%7Bvmatrix%7D%5Clambda%20-4%26-2%265%5C%5C-6%26%5Clambda-4%269%5C%5C-5%26-3%26%5Clambda%2B7%5Cend%7Bvmatrix%7D%3D0"><br/><!-- $\Rightarrow \lambda^2(\lambda-1)=0,\lambda_1=1,\lambda_2=\lambda_3=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow%20%5Clambda%5E2(%5Clambda-1)%3D0%2C%5Clambda_1%3D1%2C%5Clambda_2%3D%5Clambda_3%3D0"><br/><!-- $\lambda_1=1 \Rightarrow (E-A)x=0,E-A={\begin{bmatrix}-3&-2&5\\-6&-3&9\\-5&-3&8\end{bmatrix}}\Rightarrow{\begin{bmatrix}1&0&-1\\0&1&-1\\0&0&0\end{bmatrix}}\Rightarrow{(E-A)x=\begin{bmatrix}1&0&-1\\0&1&-1\\0&0&0\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}}=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clambda_1%3D1%20%5CRightarrow%20(E-A)x%3D0%2CE-A%3D%7B%5Cbegin%7Bbmatrix%7D-3%26-2%265%5C%5C-6%26-3%269%5C%5C-5%26-3%268%5Cend%7Bbmatrix%7D%7D%5CRightarrow%7B%5Cbegin%7Bbmatrix%7D1%260%26-1%5C%5C0%261%26-1%5C%5C0%260%260%5Cend%7Bbmatrix%7D%7D%5CRightarrow%7B(E-A)x%3D%5Cbegin%7Bbmatrix%7D1%260%26-1%5C%5C0%261%26-1%5C%5C0%260%260%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7Dx_1%5C%5Cx_2%5C%5Cx_3%5Cend%7Bbmatrix%7D%7D%3D0"><br/><!-- $\lambda_2=\lambda_3=0 \Rightarrow (Similar)$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Clambda_2%3D%5Clambda_3%3D0%20%5CRightarrow%20(Similar)"><br/>令<!-- $x_1=1,\vec{v_1}=\begin{bmatrix}1\\1\\1\end{bmatrix},\vec{v_2}=\vec{v_3}=\begin{bmatrix}1\\3\\2\end{bmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=x_1%3D1%2C%5Cvec%7Bv_1%7D%3D%5Cbegin%7Bbmatrix%7D1%5C%5C1%5C%5C1%5Cend%7Bbmatrix%7D%2C%5Cvec%7Bv_2%7D%3D%5Cvec%7Bv_3%7D%3D%5Cbegin%7Bbmatrix%7D1%5C%5C3%5C%5C2%5Cend%7Bbmatrix%7D.">
>   + 矩阵分解(Matrix Decomposition|Matrix Factorization)：
>      + 相似矩阵：同一线性变换，不同基下的矩阵称为相似矩阵.
>        |步骤||
>        |:-----|:------|
>        |<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">是<!-- $V_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=V_2">空间下的点|<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">|
>        |<!-- $\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cvec%7Bv%7D">通过可逆矩阵P变为<!-- $V_1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=V_1">空间下的点|<!-- $P\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P%5Cvec%7Bv%7D">|
>        |在<!-- $V_1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=V_1">下通过特征矩阵A完成线性变换|<!-- $AP\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=AP%5Cvec%7Bv%7D">|
>        |通过<!-- $P^{-1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P%5E%7B-1%7D">变回<!-- $V_2$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=V_2">空间下的点|<!-- $P^{-1}AP\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P%5E%7B-1%7DAP%5Cvec%7Bv%7D">|
>        |A、B均为n阶矩阵，若存在可逆矩阵P使<!-- $B=P^{-1}AP$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=B%3DP%5E%7B-1%7DAP">，则A、B为相似矩阵|<!-- $B\vec{v}=P^{-1}AP\vec{v}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=B%5Cvec%7Bv%7D%3DP%5E%7B-1%7DAP%5Cvec%7Bv%7D">|
>      + 特征分解(Eigendecomposition)：矩阵A可以对角化的话(方块矩阵)，可以通过相似矩阵进行特征值分解：<br/><!-- $A=P\Lambda P^{-1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3DP%5CLambda%20P%5E%7B-1%7D">，<br/><!-- $A=\begin{bmatrix}2&-1\\-1&2\end{bmatrix}={\begin{bmatrix}-\frac{\sqrt{2}}{2}&\frac{\sqrt{2}}{2}\\\frac{\sqrt{2}}{2}&\frac{\sqrt{2}}{2}\end{bmatrix}}{\begin{bmatrix}3&0\\0&1\end{bmatrix}}{\begin{bmatrix}-\frac{\sqrt{2}}{2}&\frac{\sqrt{2}}{2}\\\frac{\sqrt{2}}{2}&\frac{\sqrt{2}}{2}\end{bmatrix}}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3D%5Cbegin%7Bbmatrix%7D2%26-1%5C%5C-1%262%5Cend%7Bbmatrix%7D%3D%7B%5Cbegin%7Bbmatrix%7D-%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%26%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%5C%5C%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%26%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%5Cend%7Bbmatrix%7D%7D%7B%5Cbegin%7Bbmatrix%7D3%260%5C%5C0%261%5Cend%7Bbmatrix%7D%7D%7B%5Cbegin%7Bbmatrix%7D-%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%26%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%5C%5C%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%26%5Cfrac%7B%5Csqrt%7B2%7D%7D%7B2%7D%5Cend%7Bbmatrix%7D%7D">，其中<!-- $\Lambda$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CLambda">为对角矩阵，P的列向量是单位特征向量且相互正交(一个向量乘以正交矩阵的几何意义就是把该向量从当前坐标系变换到这个矩阵所表示的坐标系).<br/>对于方阵而言，矩阵不会进行维度的升降，所以矩阵代表的运动实际上只有两种：旋转和拉伸.因此特征值分解实际上是把运动分解，即A方阵既有旋转也有拉伸，特征分解后正交矩阵P与其逆矩阵<!-- $P^{-1}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=P%5E%7B-1%7D">只有旋转，对角矩阵<!-- $\Lambda$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CLambda">只有拉伸.
>      + 奇异值分解(Sigular Value Decomposition, SVD)：在实际应用中，特征向量很可能不是正交的，则需要用到奇异值分解.奇异值分解就是把矩阵分成多个"分力"；奇异值的大小就是各个"分力"的大小;奇异值分解实际上把矩阵变换分成了三部分：旋转、拉伸和投影.<br/><!-- $A=U \Sigma V^T$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3DU%20%5CSigma%20V%5ET">,A为<!-- $M \times N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=M%20%5Ctimes%20N">矩阵，U为<!-- $M \times M$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=M%20%5Ctimes%20M">正交矩阵，∑为<!-- $M \times N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=M%20%5Ctimes%20N">矩形对角矩阵，V为<!-- $N \times N$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=N%20%5Ctimes%20N">正交矩阵.<br/>A的非零奇异值为<!-- $AA^T$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=AA%5ET">或<!-- $A^T A$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%5ET%20A">的非零特征值的平方根:<br/><!-- $A=U \Sigma V^T \Leftrightarrow \begin{bmatrix}1&1\\0&1\\1&0\end{bmatrix}={\begin{bmatrix}\frac{2}{\sqrt{6}}&0&-\frac{1}{\sqrt{3}}\\\frac{1}{\sqrt{6}}&-\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{3}}\\\frac{1}{\sqrt{6}}&\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{3}}\end{bmatrix}}{\begin{bmatrix}\sqrt{3}&0\\0&1\\0&0\end{bmatrix}}{\begin{bmatrix}\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\end{bmatrix}}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3DU%20%5CSigma%20V%5ET%20%5CLeftrightarrow%20%5Cbegin%7Bbmatrix%7D1%261%5C%5C0%261%5C%5C1%260%5Cend%7Bbmatrix%7D%3D%7B%5Cbegin%7Bbmatrix%7D%5Cfrac%7B2%7D%7B%5Csqrt%7B6%7D%7D%260%26-%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%26-%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%7B6%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%5Cend%7Bbmatrix%7D%7D%7B%5Cbegin%7Bbmatrix%7D%5Csqrt%7B3%7D%260%5C%5C0%261%5C%5C0%260%5Cend%7Bbmatrix%7D%7D%7B%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cend%7Bbmatrix%7D%7D"><br/><!-- $A^TA=(V \Sigma^T U^T)(U \Sigma V^T)=V \Sigma^2 V^T$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%5ETA%3D(V%20%5CSigma%5ET%20U%5ET)(U%20%5CSigma%20V%5ET)%3DV%20%5CSigma%5E2%20V%5ET"><br/><!-- $\begin{bmatrix}1&0&1\\1&1&0\end{bmatrix}\begin{bmatrix}1&1\\0&1\\1&0\end{bmatrix}=\begin{bmatrix}2&1\\1&2\end{bmatrix}\Rightarrow \lambda_1=3,\lambda_2=1$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5Cbegin%7Bbmatrix%7D1%260%261%5C%5C1%261%260%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D1%261%5C%5C0%261%5C%5C1%260%5Cend%7Bbmatrix%7D%3D%5Cbegin%7Bbmatrix%7D2%261%5C%5C1%262%5Cend%7Bbmatrix%7D%5CRightarrow%20%5Clambda_1%3D3%2C%5Clambda_2%3D1"><br/><!-- $\Rightarrow \vec{v_1}=\begin{bmatrix}\frac{1}{\sqrt 2}\\\frac{1}{\sqrt 2}\end{bmatrix},\vec{v}_2=\begin{bmatrix}\frac{1}{\sqrt 2}\\-\frac{1}{\sqrt 2}\end{bmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow%20%5Cvec%7Bv_1%7D%3D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D%2C%5Cvec%7Bv%7D_2%3D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5C%5C-%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D"><br/><!-- $AA^T=(U\Sigma V^T)(V \Sigma^T U^T)=U \Sigma^2 U^T$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=AA%5ET%3D(U%5CSigma%20V%5ET)(V%20%5CSigma%5ET%20U%5ET)%3DU%20%5CSigma%5E2%20U%5ET"><br/><!-- $\Rightarrow \begin{bmatrix}1&1\\0&1\\1&0\end{bmatrix}\begin{bmatrix}1&0&1\\1&1&0\end{bmatrix}=\begin{bmatrix}2&1&1\\1&1&0\\1&0&1\end{bmatrix}\Rightarrow \lambda_1=3,\lambda_2=1,\lambda_3=0$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow%20%5Cbegin%7Bbmatrix%7D1%261%5C%5C0%261%5C%5C1%260%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D1%260%261%5C%5C1%261%260%5Cend%7Bbmatrix%7D%3D%5Cbegin%7Bbmatrix%7D2%261%261%5C%5C1%261%260%5C%5C1%260%261%5Cend%7Bbmatrix%7D%5CRightarrow%20%5Clambda_1%3D3%2C%5Clambda_2%3D1%2C%5Clambda_3%3D0"><br/><!-- $\Rightarrow \vec{v_1}=\begin{bmatrix}\frac{2}{\sqrt 6}\\\frac{1}{\sqrt 6}\\\frac{1}{\sqrt 6}\end{bmatrix},\vec{v}_2=\begin{bmatrix}0\\-\frac{1}{\sqrt 2}\\\frac{1}{\sqrt 2}\end{bmatrix},\vec{v}_3=\begin{bmatrix}-\frac{1}{\sqrt 3}\\\frac{1}{\sqrt 3}\\\frac{1}{\sqrt 3}\end{bmatrix}$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=%5CRightarrow%20%5Cvec%7Bv_1%7D%3D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B2%7D%7B%5Csqrt%206%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%206%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%206%7D%5Cend%7Bbmatrix%7D%2C%5Cvec%7Bv%7D_2%3D%5Cbegin%7Bbmatrix%7D0%5C%5C-%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D%2C%5Cvec%7Bv%7D_3%3D%5Cbegin%7Bbmatrix%7D-%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5Cend%7Bbmatrix%7D"><br/><!-- $A=\lambda_1 \vec{u}_1 \vec{v}_1^T+\lambda_2 \vec{u}_2 \vec{v}_2^T=\sqrt{3}\begin{bmatrix}\frac{2}{\sqrt 6}\\\frac{1}{\sqrt 6}\\\frac{1}{\sqrt 6}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt 2}&\frac{1}{\sqrt 2}\end{bmatrix}+1\begin{bmatrix}-\frac{1}{\sqrt 3}\\\frac{1}{\sqrt 3}\\\frac{1}{\sqrt 3}\end{bmatrix}\begin{bmatrix}\frac{1}{\sqrt 2}&-\frac{1}{\sqrt 2}\end{bmatrix}.$ --> <img style="transform: translateY(0.1em); background: white;" src="https://render.githubusercontent.com/render/math?math=A%3D%5Clambda_1%20%5Cvec%7Bu%7D_1%20%5Cvec%7Bv%7D_1%5ET%2B%5Clambda_2%20%5Cvec%7Bu%7D_2%20%5Cvec%7Bv%7D_2%5ET%3D%5Csqrt%7B3%7D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B2%7D%7B%5Csqrt%206%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%206%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%206%7D%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%26%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D%2B1%5Cbegin%7Bbmatrix%7D-%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5C%5C%5Cfrac%7B1%7D%7B%5Csqrt%203%7D%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%26-%5Cfrac%7B1%7D%7B%5Csqrt%202%7D%5Cend%7Bbmatrix%7D."><br/>
       \* 线性代数(马同学)[[Blog]](https://www.matongxue.com/madocs/228/)<br/>
       \* 奇异值分解(渺然如尘)[[Video]](https://www.bilibili.com/video/BV1mx411E74T?p=4)
> ----